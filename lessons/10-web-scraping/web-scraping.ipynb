{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6605f277",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "Welcome to lesson 10 of the Noisebridge Python Class! ([Noisebridge Wiki](https://www.noisebridge.net/wiki/PyClass) | [Github](https://github.com/audiodude/PythonClass))\n",
    "\n",
    "In this lesson, we will discuss web scraping. While you may not have an immediate use for the techniques outlined here, it is worthwhile to learn in order to get a better understanding of the structure of the web and the possibilities for extracting data from websites.\n",
    "\n",
    "We will be using [requests](https://requests.readthedocs.io/en/latest/) and [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/). There are lots of other solutions for web scraping out there, including those written in other languages and hosted in cloud services.\n",
    "\n",
    "In this lesson, you will learn:\n",
    "\n",
    "* How to get the raw source code of websites using their URL.\n",
    "* How to parse that source code into a data structure.\n",
    "* How to identify and refer to specific locations inside of that data structure that contain useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0221e2a",
   "metadata": {},
   "source": [
    "# The basic way the web works\n",
    "\n",
    "The Mozilla Developer Network has [an article](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/How_the_Web_works) about the basic way the web works. Basically, when you type a URL into your browser and hit 'enter', the following happens:\n",
    "\n",
    "1. The browser looks up the DNS address (a number) that corresponds to the domain in the URL.\n",
    "1. The browser sends an HTTP GET request to the server (there are other HTTP \"verbs\" other than GET as well).\n",
    "1. The server locates the [HTML](https://en.wikipedia.org/wiki/HTML) file you requested (in the case of a `example.com/foo/bar.html` type URL, where bar.html is the file) or communicates with an application to generate an HTML response, and sends it back to the browser over the [TCP/IP](https://www.cloudflare.com/learning/ddos/glossary/tcp-ip/) connection.\n",
    "1. The browser receives the response, parses it, and renders a webpage based on the received HTML along with [CSS](https://developer.mozilla.org/en-US/docs/Web/CSS) and [Javascript](https://developer.mozilla.org/en-US/docs/Web/JavaScript)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8186a",
   "metadata": {},
   "source": [
    "HTML, CSS and Javascript are the foundational pieces of the source code of the web. HTML provides the structure for a webpage, in the form of an HTML document. HTML documents are composed of tags such as `<html>` (the wrapper for the whole document) and `<a>` (a link in the document to somewhere else). CSS represents the presentation logic for the page, from things as basic as colors and margins, to complex concerns of layout and formatting. Finally, Javascript provides a programming language which allows for dynamic effects and interactions inside of a webpage. For example, when you type a query into a search engine and it provides \"auto complete\" or suggested results. Those results are retrieved and displayed with Javascript."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaeeb46",
   "metadata": {},
   "source": [
    "Let's look at a simple website from NPR, https://text.npr.org/\n",
    "\n",
    "![Screenshot of NPR basic \"text\" news site](https://pxscdn.com/public/m/_v2/588554065884192073/bf0f52ff2-92677b/e5wi2rcyLEpI/RftouyiJETFbf1rKHXWTDMArbpHfMVOtZTjfMLfw.png)\n",
    "\n",
    "We can use the program `curl` on macOS or Linux to retrieve the raw HTML contents of a web page. For example, when we run:\n",
    "\n",
    "`$ curl https://text.npr.org/`\n",
    "\n",
    "We get:\n",
    "\n",
    "![Screenshot of the result of running curl on text.npr.org](https://pxscdn.com/public/m/_v2/588554065884192073/bf0f52ff2-92677b/QcxYbe5gcGJE/pFJs6XokUP7rgLRbRz76ak3MchCMmVZqZoIILKTA.png)\n",
    "\n",
    "and if we scroll further down we see:\n",
    "\n",
    "![Screenshot of the result of running curl on text.npr.org](https://pxscdn.com/public/m/_v2/588554065884192073/bf0f52ff2-92677b/2YqKQi3pOe4z/R2N4ZlWO8ht0hmTS1mwiBXrx1a2GZPt0Xl9zFJKp.png)\n",
    "\n",
    "We see that the first news item in the web page, \"How climate change could cause a home insurance meltdown\", corresponds to HTML code in the curl response:\n",
    "\n",
    "```\n",
    "        <li><a class=\"topic-title\" href=\"/1186540332\">How climate change could cause a home insurance meltdown</a></li>\n",
    "```\n",
    "\n",
    "This is the \"raw\" code that the browser reads to create the web page seen in the first screenshot.\n",
    "\n",
    "The important thing to understand is that `curl` is retrieving the source code of the webpage in the exact same way your web browser is, except that the browser takes the extra step of turning the textual HTML data into a graphical web page.\n",
    "\n",
    "The `<li>` tag means that the article is part of a list, and the `<a>` tag means that the article name should be rendered as as link, which leads to the article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e717dce2",
   "metadata": {},
   "source": [
    "## Fetching web site code with Python\n",
    "\n",
    "We can also get the source code of the NPR text news website in Python using the `requests` library, which is for HTTP requests. The result is the same HTML code that we saw in the `curl` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b07b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "base_url = 'https://text.npr.org/'\n",
    "\n",
    "resp = requests.get(base_url)\n",
    "# Throw an exception if we get a non-success status code.\n",
    "resp.raise_for_status()\n",
    "\n",
    "code = resp.text\n",
    "\n",
    "print(code[:195])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d36f694",
   "metadata": {},
   "source": [
    "When you make a request to an HTTP server, the response includes, in it's metadata, an **HTTP status code**. You may have already heard of famous ones like \"404 NOT FOUND\" or \"500 SERVER ERROR\". You can find a list of status codes [on Wikipedia](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes).\n",
    "\n",
    "In general, 1xx status codes are for HTTP specific information. 2xx status codes generally indicate success, so we can proceed with processing the response. 3xx generally represent a **redirect**, where the HTTP server is instructing the client to make a new request to a different location (if you've ever typed a URL into your browser and wound up on a wildly different URL, that's probably a 302 redirect). The 4xx status codes are request errors, which generally mean the client did something wrong with the request such as 404 (requested something that doesn't exist) or the generic 400 (perhaps made a search request without a query or something). Finally, 5xx status codes represent something going wrong on the server, that is beyond the client's control.\n",
    "\n",
    "The `requests` method `.raise_for_status()` will raise an exception if any non-2xx status code is encountered (except for redirects, 301/302 which are followed). This prevents us from proceeding and using a possibly nonsensical (to us) response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41945ae2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e1c959",
   "metadata": {},
   "source": [
    "## Getting specific data out of the HTML\n",
    "\n",
    "Let's say we wanted to get a list of all news story headlines from this page. At this point, we could attempt to extract that data from the textual representation of the HTML (the raw code) using regex or simple string find operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c915af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "li_idx = code.find('<li>')\n",
    "li_end_idx = code.find('</li>', li_idx)\n",
    "print(code[li_idx:li_end_idx+5])\n",
    "\n",
    "next_li_idx = code.find('<li>', li_end_idx)\n",
    "next_li_end_idx = code.find('</li>', next_li_idx)\n",
    "print(code[next_li_idx:next_li_end_idx+5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e1bee",
   "metadata": {},
   "source": [
    "However this approach is tricky and error prone. In this example, it's particularly simple, because each of the `<li>` elements contains exactly one `<a>` element. However in other cases, there could be nested structures that only occur in some of the desired target elements. For this reason, we will **parse** the HTML into a **document tree** and use the tree data structure to access the data we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f895bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# The variable soup is our parsed document tree data structure.\n",
    "# 'lxml' specifies the parser library to use. It must be installed.\n",
    "soup = BeautifulSoup(code, 'lxml')\n",
    "\n",
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a14cf06",
   "metadata": {},
   "source": [
    "This is much easier, and seems very promising. We seem to have gotten all of the article links! But there's a problem, we're also getting links to the \"Terms of Use\" and \"Go to Full Site\".\n",
    "\n",
    "Now we enter the part of web scraping that is more of an art than a science, and that is finding unique identifiers for data we care about, and navigating the document tree.\n",
    "\n",
    "*Can you see a pattern with the links we care about (the news articles) that we could potentially use to extract just those?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaf2ab8",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# We use class_ with an underscore because `class` is a keyword in Python.\n",
    "articles = soup.find_all('a', class_='topic-title')\n",
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03e64e",
   "metadata": {},
   "source": [
    "At this point we need to extract the actual article titles and dispense with the markup. We can use the `.text` attribute of the returned `Tag` objects. This attribute actually abstracts some of the details of dealing with what are called \"Text Nodes\" in HTML documents, where a single logical chunk of text might be split across multiple nodes or contain embedded HTML tags. For example, this HTML:\n",
    "\n",
    "```\n",
    "<a>Hello, I <em>love</em> learning Python!</a>\n",
    "```\n",
    "\n",
    "Contains mutliple text nodes under the `<a>` tag. However, using `.text` we can get all of the text under the tag, as we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d79cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = BeautifulSoup('<a>Hello, I <em>love</em> &lt;3 learning Python!</a>')\n",
    "print(s2.a.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60543949",
   "metadata": {},
   "source": [
    "*What is a list comprehension that we could use to find all of the article title strings from the `articles` tags?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c226e32a",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Your list comprehension goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffba5e3",
   "metadata": {},
   "source": [
    "And that's it, that's web scraping. Lesson over, go home! (kidding not kidding)\n",
    "\n",
    "Imagine that twitter bots were still a thing, or better yet you wanted to post to Mastodon (as we did in lesson 1) with a post every time a new article was published to the NPR text website. You could set up a scraper that runs every 5 minutes and if it finds an article title it hasn't seen before, it creates a post with a link to the article. But how do we get the link to the article? It's in the `href` **attribute** of the `<a>` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[0].get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50326720",
   "metadata": {},
   "source": [
    "This is a **relative url**. It's not a full website address. It leads to a document that is at a path relative to the `base_url` of the website. Let's look again at `base_url`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d017831",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8393110",
   "metadata": {},
   "source": [
    "You might consider that you could simply concatenate the `base_url` to the relative URL. However this is error prone and doesn't account for particular edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff78f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url + articles[0].get('href')[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f7fe0c",
   "metadata": {},
   "source": [
    "By using a function called `urljoin` in the `urllib.parse` module, we can reconstruct the full URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259283bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "first_url = urllib.parse.urljoin(base_url, articles[0].get('href'))\n",
    "first_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1583cd94",
   "metadata": {},
   "source": [
    "We could then do:\n",
    "\n",
    "```\n",
    "post_to_mastodon('NPR: %s (%s)' % (articles[0].text, first_url))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4304bbb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ded819",
   "metadata": {},
   "source": [
    "Now let's try a more complex exercise. Let's say we want to build a data set of movie data from Rotten Tomatoes. For this example, we will consider 90s Science Fiction movies. To even begin, we will need a list of such movies, which we will get to later. First let's verify that we're able to retrieve the data we want from the Rotten Tomatoes website. Some websites are constructed dynamically using Javascript, which means that the information that eventually shows in your browser is not present in the initial HTML response. If that were the case with Rotten Tomatoes, we'd be unable to scrape the site using the methods we've discussed.\n",
    "\n",
    "Let's look at the page for the 1990 sci-fi movie *Total Recall* (https://www.rottentomatoes.com/m/total_recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa5fe8",
   "metadata": {},
   "source": [
    "![Rotten tomatoes page for Total Recall](https://pxscdn.com/public/m/_v2/588554065884192073/bf0f52ff2-92677b/PT9NMp8C3F5b/sI3iHqHumV9eGeOIFsPMr65hkMF3UTNnuhTxtknB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2d5af",
   "metadata": {},
   "source": [
    "We can see the \"Tomatometer\" rating there, as well as information such as the rating, the genre and the runtime. All of this would be useful information. Let's \"view source\" on the page (which will give us the same HTML code we would get from using `requests`), and see if we can find somewhere that this data is easily accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a08fa9",
   "metadata": {},
   "source": [
    "Just searching for the title of the movie in the source (since it appears close to the data we want) we find the following:\n",
    "\n",
    "![Screenshot of source code for Total Recall](https://pixelfed.social/storage/m/_v2/588554065884192073/530d83cd3-f15549/8N4ZQUosbxho/UriZkNKfqIvic9Xi6L7ei1sF5GWVLVQ4iyRmn96e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6a442",
   "metadata": {},
   "source": [
    "*How would we request the HTML code for the movie and select the `score-board` element?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a4354",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Request the HTML code for the Total Recall page.\n",
    "\n",
    "# Parse it with BeautifulSoup and find the <score-board> element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f6b12",
   "metadata": {},
   "source": [
    "Now we can extract some of the interesting data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d63a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find('h1', slot='titleIntro').find('span').text\n",
    "rating = soup.find('rt-text', slot='ratingsCode').text\n",
    "date = soup.find('rt-text', slot='releaseDate').text\n",
    "runtime = soup.find('rt-text', slot='duration').text\n",
    "crtics_score = soup.find('rt-button', slot='criticsScore').find('rt-text').text\n",
    "audience_score = soup.find('rt-button', slot='audienceScore').find('rt-text').text\n",
    "\n",
    "# How would you find the genres and store them in a list?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defd86b4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9df612",
   "metadata": {},
   "source": [
    "Now let's go back to the problem of getting the initial list of \"90s Science Fiction movies\". For this task, we turn to [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page), a project of the Wikimedia Foundation (the folks behind Wikipedia). Wikidata stores information about the world in a form of \"linked data\". For example, it stores that the film [Back to the Future III](https://www.wikidata.org/wiki/Q230552) has a \"country of origin\" of [United States of America](https://www.wikidata.org/wiki/Q30). It also stores that the film has a genre of \"science fiction film\" and that, most importantly, it has a Rotten Tomatoes ID of \"m/back_to_the_future_3\".\n",
    "\n",
    "![Screenshot of Wikidata entry for Back to the Future Part III](https://pixelfed.social/storage/m/_v2/588554065884192073/530d83cd3-f15549/IhUysugDr34E/EL3qhPjaNwbyzG7rZeDGPST4nU4NOnZFzw0z59Ec.png)\n",
    "\n",
    "Using the [Wikidata Query Service](https://query.wikidata.org/), we can construct a query that says \"Give me the Rotten Tomatoes ID of all science fiction films made in the 90s\". The construction of that query, which is in the SPARQL query language, will not be discussed further in this lesson. Perhaps just assume that if you were working on this project, your colleague prepared a list of Rotten Tomatoes ids for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdaef1b",
   "metadata": {},
   "source": [
    "https://w.wiki/76$A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d497351",
   "metadata": {},
   "source": [
    "<img width=600 alt=\"Screenshot of Wikidata Query Service with a query for 90s sci fi movies\" src=\"https://pxscdn.com/public/m/_v2/588554065884192073/bf0f52ff2-92677b/f6jKZtyCBm8Q/92LrBuaVJJuZPEKoSBIIvfGeP9mvRdLt6eiXl0R1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d607ab9",
   "metadata": {},
   "source": [
    "Here is the code to programatically query the Wikidata Query Service and extract the IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280387a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "query = '''\n",
    "SELECT DISTINCT ?item ?itemLabel ?rtid WHERE {\n",
    "  ?item wdt:P136 wd:Q471839.\n",
    "  ?item wdt:P577 ?pubdate.\n",
    "  ?item wdt:P1258 ?rtid.\n",
    "  FILTER((?pubdate >= \"1990-01-01T00:00:00Z\"^^xsd:dateTime) && ((?pubdate < \"2000-01-01T00:00:00Z\"^^xsd:dateTime)))\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "'''\n",
    "\n",
    "resp = requests.post('https://query.wikidata.org/sparql',\n",
    "              headers={'User-Agent': 'Noisebridge MovieBot 0.0.1/Audiodude <audiodude@gmail.com>'},\n",
    "              data={\n",
    "                'query': query,\n",
    "                'format': 'json',\n",
    "              }\n",
    ")\n",
    "resp.raise_for_status()\n",
    "\n",
    "data = resp.json()\n",
    "ids =[d['rtid']['value'] for d in data['results']['bindings']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f79742",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ids[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4325d66b",
   "metadata": {},
   "source": [
    "Once we have the IDs, we can loop over them and make the scraping request to each movie URL, storing the data in parallel lists that can be used to construct a Pandas DataFrame. Note that there is a bit of trial and error involved, in that sometimes the movie page doesn't have data for all of the things we are looking for, and we need to protect against that, but we might not know that until we try it. For this reason, we wrap the `scrape_movie` call in a `try/except` block, so that if an error is thrown we just skip that movie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0249a",
   "metadata": {},
   "source": [
    "How would we write the `scape_movie` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc564910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def scrape_movie(url):\n",
    "    return {\n",
    "        'title': None,\n",
    "        'rating': None,\n",
    "        'release_date': None,\n",
    "        'runtime': None,\n",
    "        'critics_score': None,\n",
    "        'audience_score': None,\n",
    "        'genres': None\n",
    "    }\n",
    "\n",
    "rt_base_url = 'https://www.rottentomatoes.com/'\n",
    "records = []\n",
    "\n",
    "# By wrapping the ids list in tqdm, we get a nice progress bar.\n",
    "for id_ in tqdm(ids):\n",
    "    time.sleep(1)\n",
    "    url = urllib.parse.urljoin(rt_base_url, id_)\n",
    "    try:\n",
    "        records.append(scrape_movie(url))\n",
    "    except Exception as e:\n",
    "        print(f'Could not scrape movie, id={id_}')\n",
    "        pass\n",
    "\n",
    "data = {}\n",
    "for field in ('title', 'rating', 'release_date', 'runtime', 'critics_score', 'audience_score', 'genres'):\n",
    "    data[field] = [r[field] for r in records]\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140de778",
   "metadata": {},
   "source": [
    "Finally, we can find the movies that have the highest tomatometer score, from the ones in our collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c58fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('critics_score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8239d77",
   "metadata": {},
   "source": [
    "As with all taxonomies, it appears that the definition of \"Science Fiction Film\" on Wikidata is a bit....fluid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446aacb7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6ad2f",
   "metadata": {},
   "source": [
    "## Additional considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c44de38",
   "metadata": {},
   "source": [
    "### Javascript pages\n",
    "\n",
    "Some webpages are constructed using Javascript almost exclusively. These pages might use something like the [React](https://react.dev/) framework to dynamically build the webpage after the page has first loaded. In this case, the returned HTML only contains a skeleton of the page and doesn't include the data we are after. There are potentially ways to interpret Javascript on a target page and extract the data anyways, but they are beyond the scope of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fec91a",
   "metadata": {},
   "source": [
    "### User-Agent\n",
    "\n",
    "Some websites are sensitive to being scraped. The easiest thing those sites can do to disallow scraping it to check the **User-Agent** of the program that is being used to request the data. All browsers send a User-Agent **header** as part of their requests to websites. This is a string which identifies the browser version and operating system that is making the request. Currently, on Windows 10 with Chrome the value of my User-Agent header is:\n",
    "\n",
    "```\n",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\n",
    "```\n",
    "\n",
    "By default, requests will send a User-Agent of `Python-requests/x.y.z` where `x.y.z` is the version number of the library. A website might see this and decide it's not a real web browser and deny the request.\n",
    "\n",
    "Since the User-Agent is data provided by the client (us), we can set it to anything we want. Setting the User-Agent to look like the request is coming from a web browser is called **User-Agent spoofing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e8063",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\n",
    "    'https://text.npr.org/',\n",
    "    headers={\n",
    "        'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "        '(KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059c1aa4",
   "metadata": {},
   "source": [
    "If the website doesn't seem sensitive to User-Agent headers, another thing that you could do is send a User-Agent that identifies yourself or your bot. This is particularly courteous because it allows the operator of the website to identify you in the case that your bot is causing problems. Wikipedia, for example, has a [User-Agent policy](https://meta.wikimedia.org/wiki/User-Agent_policy) that requires bot operators and those running automated scripts to identify themselves in the User-Agent string.\n",
    "\n",
    "```\n",
    "MovieBot 1.0/<audiodude@gmail.com>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face2ba5",
   "metadata": {},
   "source": [
    "### Rate limiting\n",
    "\n",
    "The most common mistake beginners make when performing web scraping is to \"hammer\" the target website. You might have a loop that requests every news article from the front page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d80607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_url(relative_url):\n",
    "    return urllib.parse.urljoin(base_url, relative_url)\n",
    "\n",
    "all_absolute_urls = [absolute_url(a.get('href')) for a in articles]\n",
    "for url in all_absolute_urls:\n",
    "    # This will make the requests as fast as possible, potentially multiple per second\n",
    "    resp = requests.get(url)\n",
    "    resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c71643",
   "metadata": {},
   "source": [
    "As the comment says, if you run this code your computer will make network requests to the target website as fast as possible. Many websites will see this flood of requests and either recognize it as a scraping procedure, or even consider it a Denial of Service attack. This could lead to your IP address being blocked. The solution is to intentionally delay or **sleep** your code between requests. Although this might seem annoying it is both courteous to the website you're scraping and will prevent you from getting blocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c240b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def absolute_url(relative_url):\n",
    "    return urllib.parse.urljoin(base_url, relative_url)\n",
    "\n",
    "all_absolute_urls = [absolute_url(a.get('href')) for a in articles]\n",
    "for url in all_absolute_urls:\n",
    "    # Sleep for two seconds before making each request.\n",
    "    time.sleep(2)\n",
    "    resp = requests.get(url)\n",
    "    resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c4ee23",
   "metadata": {},
   "source": [
    "### IP Address\n",
    "\n",
    "You've probably heard of IP Addresses. They're like your phone number on the Internet. Although most home internet providers use rotating IP addresses, it is likely that your home IP address will stay the same for a relatively long period (measured in weeks).\n",
    "\n",
    "If you get banned from scraping a website, they will most likely do so by using your IP address, so that when they see a request from your IP, they will drop it or return an error code. This can look like requests that hang indefinitely.\n",
    "\n",
    "There are web scraping services that will allow you to connect to a **proxy**, which will cause your IP address to be different every time you make a request to the website you are scraping. This will let you avoid IP bans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc029d",
   "metadata": {},
   "source": [
    "### Pagination\n",
    "\n",
    "Sometimes, the data you want will be spread across multiple \"pages\" of an app. We have a couple of options in this case. We could inspect the HTML of the page for the link to \"next page\" and use the URL of that link to visit the next page of results. We could also programatically construct the URLS by figuring out the pagination scheme. For example if the urls are like:\n",
    "\n",
    "```\n",
    "https://example.com/forum/posts?page=1\n",
    "```\n",
    "\n",
    "We could increment the page value to 2, 3, etc. Then we could request each of these URLs in order to scrape the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05874a",
   "metadata": {},
   "source": [
    "### Bypassing HTML\n",
    "\n",
    "It's possible that a page you are trying to scrape doesn't use Javascript exclusively (see above) but might use it incrementally, for example with some \"infinite scroll\" behavior on a search results page. In these cases, you can often use the \"Network\" tab of the excellent [Chrome DevTools](https://developer.chrome.com/docs/devtools/) to figure out what network request the page is making. For example, if you are on `https://example.com/search?q=cats`, you might see that the page is making requests to `https://api.example.com/v1/search?q=cats&page=2`. In this case, you can run your scraper against the \"backend\" URL directly to retrieve the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ab0b07",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1013bb",
   "metadata": {},
   "source": [
    "And that's it! Scraping is a large and varied topic, but we've touched on many of the major processes and concerns. Real companies use web scraping every day to collect data that they use in their products, or to survey competitors. You can imagine that the basic techniques in this article could be expanded upon further to build production ready scraping pipelines."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": null,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
